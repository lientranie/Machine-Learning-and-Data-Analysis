{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "Colors = ['#004165','#dc4200',  '#69be28', '#f0be00', '#0cc6de', '#bed600', '#006983', '2b580c', '639a67']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "#from adspy_shared_utilities import load_crime_dataset\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "\n",
    "# synthetic dataset for simple regression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X, y = make_regression(n_samples = 1000, n_features=1, n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "\n",
    "plt.scatter(X, y, marker= 'o', color = \"darkred\", s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "A linear model makes a prediction by simpy computing a weighted sum of the input features. Whereas the constant $\\alpha_0$ is called the intercept term of bias term:\n",
    "\n",
    "$$ y^* = \\sum_{k=0}^n \\alpha_k x_k =  \\alpha \\cdot x,$$\n",
    "\n",
    "where $y^*$ us the predicted value. If $\\alpha$ and $x$ are column vectors, we write $y^* = \\alpha ^T x$. \\p \n",
    "\n",
    "The most common performance measure of a regression model is the Rooth Mean Square Error (RMSE). In particular, linear regression model looks for the coefficients $\\alpha$ that minimizes the RMSE term. The MSE cost function for a Linear Regression model:\n",
    "$$ MSE(X, h_\\alpha) = \\frac{1}{m} \\sum_{k=1}^n (\\alpha ^T x - y^*)^2 $$\n",
    "\n",
    "### The Normal Equation\n",
    "\n",
    "In order to find the value of the coefficient $\\alpha$ that minimizes the cost function, there is a closed form solution or a mathematical equation called Normal equation:\n",
    "$$ \\hat{\\alpha} = (X^TX)^T X^Ty,\n",
    "$$\n",
    "where,\n",
    "- $\\hat{\\alpha}$ is the value of $\\alpha$ that minimizes the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([150.77632751,  82.01323352])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((len(X), 1)), X]  # add x0 = 1 to each instance\n",
    "alpha_hat = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "alpha_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150.77632751328946, array([82.01323352]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now we try the Linear Regression from sklearn \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "linreg.intercept_, linreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear Regression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([150.77632751,  82.01323352])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "alpha_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"svd\" stands for the singular value decomposition of matrix $X$. The above function compute\n",
    "$$ \\hat{\\alpha} =  X^+ y,$$\n",
    "where $X^+$ is the pseudoinverse of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression using batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_bgd = []\n",
    "\n",
    "def plot_gradient_descent(theta, eta, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_sgd = []\n",
    "m = len(X_b)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "plt.figure()\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        if epoch == 0 and i < 20:                    # not shown in the book\n",
    "            y_predict = X_new_b.dot(theta)           # not shown\n",
    "            style = \"b-\" if i > 0 else \"r--\"         # not shown\n",
    "            plt.plot(X_new, y_predict, style)        # not shown\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 # not shown\n",
    "\n",
    "plt.plot(X, y, \"b.\")                                 # not shown\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)                     # not shown\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           # not shown\n",
    "plt.axis([0, 2, 0, 15])                              # not shown\n",
    "plt.show()                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_mgd = []\n",
    "\n",
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = 0\n",
    "for epoch in range(n_iterations):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)\n",
    "        \n",
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "theta_path_mgd = np.array(theta_path_mgd)\n",
    "plt.figure()\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
    "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
    "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
    "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.figure()\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "plt.figure()\n",
    "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    plt.figure()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "   \n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
    "    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.axis([0, 80, 0, 3])                         # not shown in the book\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.axis([0, 80, 0, 3])           # not shown\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 20\n",
    "X = 3 * np.random.rand(m, 1)\n",
    "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "X_new = np.linspace(0, 3, 100).reshape(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
    "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
    "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
    "        if polynomial:\n",
    "            model = Pipeline([\n",
    "                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "                    (\"std_scaler\", StandardScaler()),\n",
    "                    (\"regul_reg\", model),\n",
    "                ])\n",
    "        model.fit(X, y)\n",
    "        y_new_regul = model.predict(X_new)\n",
    "        lw = 2 if alpha > 0 else 1\n",
    "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
    "    plt.plot(X, y, \"b.\", linewidth=3)\n",
    "    plt.legend(loc=\"upper left\", fontsize=15)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 3, 0, 4])\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(122)\n",
    "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(122)\n",
    "plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = deepcopy(sgd_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
    "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
    "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n",
    "\n",
    "t1s = np.linspace(t1a, t1b, 500)\n",
    "t2s = np.linspace(t2a, t2b, 500)\n",
    "t1, t2 = np.meshgrid(t1s, t2s)\n",
    "T = np.c_[t1.ravel(), t2.ravel()]\n",
    "Xr = np.array([[1, 1], [1, -1], [1, 0.5]])\n",
    "yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n",
    "\n",
    "J = (1/len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis=1)).reshape(t1.shape)\n",
    "\n",
    "N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\n",
    "N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n",
    "\n",
    "t_min_idx = np.unravel_index(np.argmin(J), J.shape)\n",
    "t1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n",
    "\n",
    "t_init = np.array([[0.25], [-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd_path(theta, X, y, l1, l2, core = 1, eta = 0.05, n_iterations = 200):\n",
    "    path = [theta]\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = core * 2/len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + l2 * theta\n",
    "        theta = theta - eta * gradients\n",
    "        path.append(theta)\n",
    "    return np.array(path)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n",
    "for i, N, l1, l2, title in ((0, N1, 2., 0, \"Lasso\"), (1, N2, 0,  2., \"Ridge\")):\n",
    "    JR = J + l1 * N1 + l2 * 0.5 * N2**2\n",
    "    \n",
    "    tr_min_idx = np.unravel_index(np.argmin(JR), JR.shape)\n",
    "    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n",
    "\n",
    "    levelsJ=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)\n",
    "    levelsJR=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)\n",
    "    levelsN=np.linspace(0, np.max(N), 10)\n",
    "    \n",
    "    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n",
    "    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n",
    "    path_N = bgd_path(np.array([[2.0], [0.5]]), Xr, yr, np.sign(l1)/3, np.sign(l2), core=0)\n",
    "\n",
    "    ax = axes[i, 0]\n",
    "    ax.grid(True)\n",
    "    ax.axhline(y=0, color='k')\n",
    "    ax.axvline(x=0, color='k')\n",
    "    ax.contourf(t1, t2, N / 2., levels=levelsN)\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.set_title(r\"$\\ell_{}$ penalty\".format(i + 1), fontsize=16)\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$\\theta_2$\", fontsize=16, rotation=0)\n",
    "\n",
    "    ax = axes[i, 1]\n",
    "    ax.grid(True)\n",
    "    ax.axhline(y=0, color='k')\n",
    "    ax.axvline(x=0, color='k')\n",
    "    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n",
    "    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.plot(t1r_min, t2r_min, \"rs\")\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\", fontsize=20)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
    "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=10**10, random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
    "\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.5, \"Not Iris virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
